{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f96ac72d",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "68f08fa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement cassis (from versions: none)\n",
      "ERROR: No matching distribution found for cassis\n",
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install cassis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f28da72",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cassis import *\n",
    "import os\n",
    "test_set_ids = ['lusa_97',\n",
    " 'lusa_4',\n",
    " 'lusa_67',\n",
    " 'lusa_20',\n",
    " 'lusa_83',\n",
    " 'lusa_104',\n",
    " 'lusa_80',\n",
    " 'lusa_79',\n",
    " 'lusa_34',\n",
    " 'lusa_47',\n",
    " 'lusa_30',\n",
    " 'lusa_96',\n",
    " 'lusa_11',\n",
    " 'lusa_112',\n",
    " 'lusa_100',\n",
    " 'lusa_77',\n",
    " 'lusa_38',\n",
    " 'lusa_86',\n",
    " 'lusa_60']\n",
    "\n",
    "#move test_files to evaluation folder\n",
    "\n",
    "def load_aligned_files(docs_path):\n",
    "\taligner_files_names = []\n",
    "\taligner_files = []\n",
    "\taligner_files_dirs = []\n",
    "\tfor file_ in os.listdir(docs_path):\n",
    "\t\tfile__ = file_.strip(\".zip\")\n",
    "\t\tif file__ in test_set_ids:\n",
    "\t\t\tfor file in os.listdir(os.path.join(docs_path, file_)):\n",
    "\t\t\t\tif file == \"INITIAL_CAS.json\":\n",
    "\t\t\t\t\taligner_files_dirs.append((os.path.join(docs_path, file_ +\"/\"+ file), file__))\n",
    "\t\t\t\t\twith open(os.path.join(docs_path, file_ +\"/\"+ file), \"r\") as f:\n",
    "\t\t\t\t\t\tcas = load_cas_from_json(f)\n",
    "\t\t\t\t\t\taligner_files.append(cas)\n",
    "\t\t\t\t\t\taligner_files_names.append(file__)\n",
    "\n",
    "\treturn aligner_files\n",
    "Mtrans_path = \"./no_Mtrans/annotation\"\n",
    "lemma_path = \"./no_lemma/annotation\"\n",
    "fuzzy_path = \"./no_fuzzy/annotation\"\n",
    "wAligner_path = \"./no_wAligner/annotation\"\n",
    "overall_path = \"../validation/lingualigner/\"\n",
    "\n",
    "mtrans_files = load_aligned_files(Mtrans_path)\n",
    "lemma_files = load_aligned_files(lemma_path)\n",
    "fuzzy_files = load_aligned_files(fuzzy_path)\n",
    "wAligner_files = load_aligned_files(wAligner_path)\n",
    "overall_files = load_aligned_files(overall_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e2cbc74",
   "metadata": {},
   "source": [
    "### Load Reviewed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe8a7f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "nana_files = []\n",
    "nana_path = \"../validation/nana/annotation\"\n",
    "nana_files_names = []\n",
    "for file_ in os.listdir(nana_path):\n",
    "\tfile__ = file_.strip(\".zip\")\n",
    "\tif file__ in test_set_ids:\n",
    "\t\tfor file in os.listdir(os.path.join(nana_path, file_)):\n",
    "\t\t\tif file == \"storysense_anotador.json\":\n",
    "\t\t\t\twith open(os.path.join(nana_path, file_ +\"/\"+ file), \"r\") as f:\n",
    "\t\t\t\t\tcas = load_cas_from_json(f)\n",
    "\t\t\t\t\tnana_files.append(cas)\n",
    "\t\t\t\t\tnana_files_names.append(file__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "be17b15b",
   "metadata": {},
   "outputs": [],
   "source": [
    "mtrans_spans = [cas.select(\"custom.Span\") for cas in mtrans_files]\n",
    "lemma_spans = [cas.select(\"custom.Span\") for cas in lemma_files]\n",
    "fuzzy_spans = [cas.select(\"custom.Span\") for cas in fuzzy_files]\n",
    "wAligner_spans = [cas.select(\"custom.Span\") for cas in wAligner_files]\n",
    "overall_spans = [cas.select(\"custom.Span\") for cas in overall_files]\n",
    "nana_spans = [cas.select(\"custom.Span\") for cas in nana_files]\n",
    "\n",
    "#sample = aligner_spans[0][0]\n",
    "#sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0817690",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mtrans_spans_dicts = [dict((span.xmiID, span) for span in spans) for spans in mtrans_spans]\n",
    "lemma_spans_dicts = [dict((span.xmiID, span) for span in spans) for spans in lemma_spans]\n",
    "fuzzy_spans_dicts = [dict((span.xmiID, span) for span in spans) for spans in fuzzy_spans]\n",
    "wAligner_spans_dicts = [dict((span.xmiID, span) for span in spans) for spans in wAligner_spans]\n",
    "overall_spans_dicts = [dict((span.xmiID, span) for span in spans) for spans in overall_spans]\n",
    "nana_spans_dicts = [dict((span.xmiID, span) for span in spans) for spans in nana_spans]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "3b065f6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exact match removing M_Trans: 60.43\n",
      "Exact match removing Lemma: 75.72\n",
      "Exact match removing Fuzzy: 72.12\n",
      "Exact match removing Word_aligner: 68.6\n",
      "Exact match removing Overall: 75.96\n"
     ]
    }
   ],
   "source": [
    "#get text for each  nana and aligner spans (with same id) and calculate f1 score on token level\n",
    "def calculate_exact_match(aligner_spans_dicts, nana_spans_dicts):\n",
    "\terrors = 0\n",
    "\tcorrects = 0\n",
    "\ttotal = 0\n",
    "\tfor aligner_dict, nana_dict in zip(aligner_spans_dicts, nana_spans_dicts):\n",
    "\t\tfor xmi_id in aligner_dict.keys():\n",
    "\t\t\tif xmi_id in nana_dict:\n",
    "\t\t\t\taligner_span = aligner_dict[xmi_id]\n",
    "\t\t\t\tnana_span = nana_dict[xmi_id]\n",
    "\t\t\t\tif nana_span.Aligned == True:\n",
    "\t\t\t\t\tif nana_span.begin == aligner_span.begin and nana_span.end == aligner_span.end:\n",
    "\t\t\t\t\t\tcorrects += 1\n",
    "\t\t\t\t\telse:\n",
    "\t\t\t\t\t\terrors += 1\n",
    "\t\t\t\ttotal += 1\n",
    "\treturn errors, corrects, total\n",
    "\n",
    "mtrans_errors, mtrans_corrects, mtrans_total = calculate_exact_match(mtrans_spans_dicts, nana_spans_dicts)\n",
    "lemma_errors, lemma_corrects, lemma_total = calculate_exact_match(lemma_spans_dicts, nana_spans_dicts)\n",
    "fuzzy_errors, fuzzy_corrects, fuzzy_total = calculate_exact_match(fuzzy_spans_dicts, nana_spans_dicts)\n",
    "wAligner_errors, wAligner_corrects, wAligner_total = calculate_exact_match(wAligner_spans_dicts, nana_spans_dicts)\n",
    "overall_errors, overall_corrects, overall_total = calculate_exact_match(overall_spans_dicts, nana_spans_dicts)\n",
    "\n",
    "#print exact match results\n",
    "\n",
    "mtrans_exact_match_percentage = round((mtrans_corrects / mtrans_total) * 100, 2)\n",
    "print(\"Exact match removing M_Trans:\", mtrans_exact_match_percentage)\n",
    "lemma_exact_match_percentage = round((lemma_corrects / lemma_total) * 100, 2)\n",
    "print(\"Exact match removing Lemma:\", lemma_exact_match_percentage)\n",
    "fuzzy_exact_match_percentage = round((fuzzy_corrects / fuzzy_total) * 100, 2)\n",
    "print(\"Exact match removing Fuzzy:\", fuzzy_exact_match_percentage)\n",
    "wAligner_exact_match_percentage = round((wAligner_corrects / wAligner_total) * 100, 2)\n",
    "print(\"Exact match removing Word_aligner:\", wAligner_exact_match_percentage)\n",
    "overall_exact_match_percentage = round((overall_corrects / overall_total) * 100, 2)\n",
    "print(\"Exact match removing Overall:\", overall_exact_match_percentage)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
