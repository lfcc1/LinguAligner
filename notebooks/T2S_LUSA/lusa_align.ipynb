{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load T2S Lusa corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = \"../../Datasets/lusa_news/\"\n",
    "import os\n",
    "\n",
    "files = []\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.endswith(\".txt\") and \"en\" not in filename:\n",
    "        files.append(filename.split(\".\")[0])  \n",
    "        \n",
    "len(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = files[0]\n",
    "file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse Brat format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from brat_parser import get_entities_relations_attributes_groups\n",
    "\n",
    "def parseBratFile(file):\n",
    "    entities, relations, attributes, groups = get_entities_relations_attributes_groups(file)\n",
    "    return entities, relations, attributes, groups\n",
    "\n",
    "\n",
    "def filter_annotations(annotations, type, match=False, exceptions=[]):\n",
    "    if match:\n",
    "        return {id: ann for id, ann in annotations.items() if type in ann.type and ann.type not in exceptions}\n",
    "    else:\n",
    "        return {id: ann for id, ann in annotations.items() if ann.type == type and ann.type not in exceptions}\n",
    "    \n",
    "\n",
    "def get_entities_translations(entities,entities_translations):\n",
    "    res = {}\n",
    "    for id_, entity in entities.items():\n",
    "        entity.translation = entities_translations[id_].text\n",
    "        res[id_] = entity\n",
    "    return res\n",
    "\n",
    "entities, relations, attributes, groups = get_entities_relations_attributes_groups(directory+ file + \".ann\")\n",
    "\n",
    "entities_translations, _, _, _ = get_entities_relations_attributes_groups(directory+ \"translated_lusa/en/\" + file + \".ann\")\n",
    "\n",
    "entities = get_entities_translations(entities,entities_translations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_spans(file):\n",
    "    spans = []\n",
    "    start_index = 0 \n",
    "    with open(file) as f:\n",
    "        for line in f:\n",
    "            end_index = start_index + len(line)\n",
    "            spans.append((line, (start_index, end_index)))\n",
    "            start_index = end_index\n",
    "    return spans\n",
    "\n",
    "\n",
    "    \n",
    "def get_sentence_entities(span,entities):\n",
    "    res = []\n",
    "    for k, entity in entities.items():\n",
    "        if entity.span[0][0] >= span[0] and entity.span[0][1] <= span[1]:\n",
    "            res.append(entity)\n",
    "    return res\n",
    "\n",
    "\n",
    "\n",
    "def process_file(files):\n",
    "    \n",
    "    entries = {}\n",
    "    for file in files:\n",
    "        #print(file)\n",
    "        entities, relations, attributes, groups = get_entities_relations_attributes_groups(directory+ file + \".ann\")\n",
    "        #print(\"parsed original\")\n",
    "        entities_translations, _, _, _ = get_entities_relations_attributes_groups(directory+ \"translated_lusa/en/\" + file + \".ann\")\n",
    "        entities = get_entities_translations(entities,entities_translations)\n",
    "        #print(\"parsed translated\")\n",
    "        \n",
    "        spans = sentence_spans(directory+ file + \".txt\")\n",
    "\n",
    "        spans_trasnlated = sentence_spans(directory+ \"translated_lusa/en/\" + file + \".txt\")\n",
    "\n",
    "        if len(spans) != len(spans_trasnlated):\n",
    "            print(\"error\")\n",
    "        dict_sents = {}\n",
    "        for i, (sentence, span) in enumerate(spans):\n",
    "            dict_sents[span] = {\n",
    "                \"src_sent\": sentence, \n",
    "                \"src_anns\": get_sentence_entities(span, entities),\n",
    "                \"tgt_sent\": spans_trasnlated[i][0],\n",
    "                \"sent_tg_span\": spans_trasnlated[i][1]\n",
    "                }\n",
    "        entry = {\n",
    "            \"sents\": dict_sents,\n",
    "            \"entities\": entities,\n",
    "            \"relations\": list(relations.values()),\n",
    "            \"attributes\": list(attributes.values()),\n",
    "            \"groups\": list(groups.values())\n",
    "        }\n",
    "        entries[file] = entry\n",
    "    return entries\n",
    "\n",
    "entries = process_file(files)\n",
    "len(entries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load LinguAligner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from LinguAligner import AlignmentPipeline, translation\n",
    "\n",
    "config= {\n",
    "    \"pipeline\": [ \"lemma\", \"M_Trans\", \"word_aligner\",\"gestalt\",\"leveinstein\"], # can be changed according to the desired pipeline\n",
    "    \"spacy_model\": \"en_core_web_sm\", # needed for lemma\n",
    "    \"WAligner_model\": \"bert-base-multilingual-uncased\", # needed for word_aligner\n",
    "}\n",
    "aligner = AlignmentPipeline(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating lookup Table (Multiple Translations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "annotations = set()\n",
    "\n",
    "for entry in entries:\n",
    "    for span, sent in entry[\"sents\"].items():\n",
    "        for ann in sent[\"src_anns\"]:\n",
    "            annotations.add(ann.text)\n",
    "print(len(annotations), annotations )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lookupTable = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translator = translation.MicrosoftTranslator(source_lang=\"pt\", target_lang=\"en\", auth_key=\"f6d44239a73046ca8378bcdc689b395c\")\n",
    "for word in annotations:\n",
    "    if word not in lookupTable and len(word) < 50:\n",
    "        mtrans = translator.getMultipleTranslations(word)\n",
    "        lookupTable[word] = mtrans\n",
    "        print(word, mtrans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import json\n",
    "#f_out = open(\"lookupTable_annotations.json\", \"w\")\n",
    "#json.dump(lookupTable, f_out, indent=4, ensure_ascii=False)\n",
    "#f_out.flush()\n",
    "#f_out.close()\n",
    "\n",
    "import json\n",
    "lookupTable = json.load(open(\"lookupTable_annotations.json\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using LinguAligner to align translated annotations with the translated text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "for file, entry in entries.items():\n",
    "    for span, sent in entry[\"sents\"].items():\n",
    "        for ann in sent[\"src_anns\"]:\n",
    "            target_annotation = aligner.align_annotation(sent[\"src_sent\"], ann.text, sent[\"tgt_sent\"], ann.translation, lookupTable=lookupTable, src_ann_start=ann.span[0][0])\n",
    "            ann.tgt_ann = target_annotation\n",
    "    i += 1\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pickle import dump, load\n",
    "#dump(entries, open(\"lusa_entries.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save data in Brat format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entries = load(open(\"lusa_entries.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for file, entry in entries.items():\n",
    "    entry[\"entities\"] = []\n",
    "    for span, sent in entry[\"sents\"].items():\n",
    "        for ann in sent[\"src_anns\"]:\n",
    "            print(ann.text, ann.span, ann.translation, ann.tgt_ann)\n",
    "            ann.text = ann.tgt_ann[0]\n",
    "            ann.span = ((ann.tgt_ann[1][0] + sent[\"sent_tg_span\"][0], ann.tgt_ann[1][1] + sent[\"sent_tg_span\"][0] + 1),)\n",
    "            entry[\"entities\"].append(ann)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read a brat file, swap every entity with a new entity and write a new file with the new entities\n",
    "def get_entity(id,entities):\n",
    "    found = None\n",
    "    res = 0\n",
    "    for entity in entities:\n",
    "        #print(entity.id, id)\n",
    "        if entity.id == id:\n",
    "            found =  entity\n",
    "    if not found:\n",
    "\n",
    "        print(\"not found\")\n",
    "    return found\n",
    "\n",
    "\n",
    "def swap_entities(file, entry, output_file):\n",
    "    with open(file) as f:\n",
    "        with open(output_file, \"w\") as f_out:\n",
    "            for line in f:\n",
    "                if line.startswith(\"T\"):\n",
    "                    id = line.strip().split(\"\\t\")[0]\n",
    "                    entity = get_entity(id, entry[\"entities\"])\n",
    "                    f_out.write(f\"{entity.id}\\t{entity.type} {entity.span[0][0]} {entity.span[0][1]}\\t{entity.text}\\n\")\n",
    "                    #print(entity.text)\n",
    "                else:\n",
    "                    f_out.write(line)\n",
    "                    ...\n",
    "                    \n",
    "for file in files:\n",
    "    swap_entities(directory + file + \".ann\", entries[file], directory+ \"translated_lusa/en_aligned/\" + file + \".ann\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#copy text files crom translated_lusa/en to translated_lusa/en_aligned\n",
    "import shutil\n",
    "for file in files:\n",
    "    shutil.copy(directory + \"translated_lusa/en/\" + file + \".txt\", directory+ \"translated_lusa/en_aligned/\" + file + \".txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in files[0:1]:\n",
    "    print(file)\n",
    "    entities, relations, attributes, groups = get_entities_relations_attributes_groups(directory+ \"translated_lusa/en_aligned/\" +file + \".ann\")\n",
    "    print(entities)\n",
    "\n",
    "    #print(\"parsed translated\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
